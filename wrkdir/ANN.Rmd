---
title: "R Notebook"
output: html_notebook
---


## Problem1— Using ANN for Classifying News Articles
For this problem, we are going to use reuters dataset. It is a set of short newswires along with their topics
published by Reuters in 1986. It is a simple, widely used toy dastaset for text classification. You are
going to use a simple feedforward neural network to classify reuters news into 46 different
topics/classes.
The reuters dataset comes packaged as part of Keras . Import the dataset from keras using the following
commands:
```{r}
library(keras)
reuters = dataset_reuters(num_words=10000)
```

Take a look at the structure of reuters.dataset. The dataset is already pre-processed and split into train
and test sets. Each example in the train or test set represents a news article and is stored as a list of
integers where each integer represents the index of a specific word appearing in that article. When you
set num_words=1000 in dataset_reuters, then keras keeps only the top 10000 most frequent words in the
data. Let’s take a look at the structure of the training examples:

```{r}
str(reuters$train$x)
```

This means that the first article consists of the words with indices 1,2,2,8,etc.
As you can see, the examples in the train and test data are vectors of varying lengths. Neural Networks
requires a matrix where each row represents an example and each column represent an attribute of that
example. Therefore, to prepare this data for neural networks we need to make sure that all examples
have the same number of columns ( that is, they are vectors of the same length). To do this, we do one-
hot encoding of each example to turn them into vectors of 0s and 1s. This would mean for instance,
turning the sequence [1,2,2,8,....] into a 10000 dimensional vector that would be all zeros except for
indices 1,2,8, etc. In other words, we turn each news article into a 10000 dimensional binary vector
which indicates which of the top 10000 frequent words occur in that article. You can use the following
r function to do this one-hot-encoding:

```{r}
one_hot_encoding = function(x, dimension=10000) {
  encoded = matrix(0, length(x), dimension)
  for (i in 1:length(x))
  encoded[i, x[[i]]] = 1
  encoded
}
```

Where x is the list of examples for which you want to do one-hot-encoding. Call this function on
reuters\$train\$x and reuters\$test\$x to get the one-hot encoding of the train and test examples. This will
produce a matrix with dimensions (8982,10000) for the train data and a matrix with dimensions(2246,
10000) for the test data. You can now use these matrices to train and test your neural network model.

```{r}
reuters_train = one_hot_encoding(reuters$train$x)
reuters_test = one_hot_encoding(reuters$test$x)
```

__Q1. (5 pts)__ Create an ANN model to classify reuters news article into 46 classes (note that
reuters\$train\$y and reuters\$test\$y are vectors of integers between 0-45 representing the category of
each news article). Use at least two hidden layers and compute the accuracy of your model on the test
set.
Unlike the fashion Mnist dataset where each image had to be flattened into a one dimensional vector,
for the reuters dataset, you do not need the flatten layer before the dense layer as each observation
(news article) is already one dimensional.

```{r}
model <- keras_model_sequential()
model %>%
        layer_dense( units = 50, activation = "relu", input_shape = 10000) %>%
        layer_dense( units = 50, activation = "relu") %>%
        layer_dense( units = 50, activation = "relu") %>%
        layer_dense(units = 46, activation = 'softmax')
```
```{r}
set.seed(123)
model %>% compile (
     loss = 'sparse_categorical_crossentropy',
     optimizer = 'adam',
     metrics = 'accuracy')
```
```{r}
history = model %>% fit (x = reuters_train, y = reuters$train$y)
```
```{r}
score <- model %>% evaluate(reuters_test, reuters$test$y, verbose = 2)

cat('Test accuracy:', score[2], "\n")
```

__Q2. (5 pts)__ Split the train data into train/validation set. Use the first 1000 examples in reuters$train for
validation and the rest for training your model. Use “tfruns” package to tune your ANN’s hyperparameters including, the number of nodes in each hidden layer, the activation function in each hidden
layer, batch_size, learning_rate, and the number of epochs). Validate each model on the validation set.  
Answer the following questions:  
      1- Which model ( which hyper-parameter combination) resulted in the best accuracy on the
      validation data? Make sure that you print the returned value from tfruns and report the  
      run with the highest validation accuracy.  
      2- Print the learning curve of your best model. Does your best model still overfit?  
      3- Does your validation_loss stop decreasing after several epochs? If so, at roughly which epoch  
      does your validation_loss stop decreasing?

```{r}
reuters_validation_x = one_hot_encoding(reuters$train$x[1:1000])
validation_labels = reuters$train$y[1:1000]

reuters_train = one_hot_encoding(reuters$train$x[1001:length(reuters$train$x)])
reuters_train_labels = reuters$train$y[1001:length(reuters$train$y)]
```
```{r}
library(tfruns)
```
```{r}
runs <- tuning_run("reuters.R",
                   flags = list (
                     nodes = c(64, 128, 392),
                     learning_rate = c(0.01, 0.05, 0.001, 0.0001),
                     batch_size = c(100, 200, 500, 1000),
                     epochs = c(30,50,100),
                     activation = c("relu","sigmoid","tanh")
                   ), sample = 0.02)
```
```{r}
runs
```
1. The model with the best validation accuracy was the model from the first run
2. see attached for plot of best training run
3. the validation error stops decreasing after the 13th epoch

__Q3. (5 pts)__ Now use ALL the training data in reuters$train (i.e., train + validation data) to train an
ANN with the best hyper- parameter combination you found after tuning in the previous question.
Compute the accuracy of this model on the test set.

```{r}
reuters_train = one_hot_encoding(reuters$train$x)
reuters_test = one_hot_encoding(reuters$test$x)
```

```{r}
model <- keras_model_sequential()
model %>%
        layer_dense( units = 128, activation = "tanh", input_shape = 10000) %>%
        layer_dense( units = 128, activation = "tanh") %>%
        layer_dense( units = 128, activation = "tanh") %>%
        layer_dense(units = 46, activation = 'softmax')
```

```{r}
set.seed(123)
model %>% compile (
        loss = 'sparse_categorical_crossentropy',
        optimizer = optimizer_adam(lr=0.0001),
        metrics = 'accuracy')

model %>% fit (x = reuters_train, y = reuters$train$y, epochs = 30, batch_size = 100)
```

```{r}
history = model %>% fit (x = reuters_train, y = reuters$train$y)
```
```{r}
history
```




